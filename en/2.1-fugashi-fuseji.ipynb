{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yr42C927w9w7"
   },
   "source": [
    "## 2.1 An Introduction to fugashi \n",
    "\n",
    "\n",
    "In this section you'll learn how to do Japanese tokenization using fugashi, a MeCab wrapper, and the unidic-lite dictionary. \n",
    "\n"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|surface|pos1|pos2|pos3|lemma|pron|kana|goshu|\n",
    "|-------|----|----|----|-----|----|----|-----|\n",
    "|喫茶|名詞|普通名詞|一般|喫茶|キッサ|キッサ|漢|\n",
    "|店|接尾辞|名詞的|一般|店|テン|テン|漢|\n",
    "|と|助詞|格助詞|*|と|ト|ト|和|\n",
    "|カフェ|名詞|普通名詞|一般|カフェ-cafe|カフェ|カフェ|外|\n",
    "|の|助詞|格助詞|*|の|ノ|ノ|和|\n",
    "|違い|名詞|普通名詞|一般|違い|チガイ|チガイ|和|\n",
    "|は|助詞|係助詞|*|は|ワ|ハ|和|\n",
    "|意外|形状詞|一般|*|意外|イガイ|イガイ|漢|\n",
    "|と|助詞|格助詞|*|と|ト|ト|和|\n",
    "|明確|形状詞|一般|*|明確|メーカク|メイカク|漢|\n",
    "\n",
    "\n",
    "This table is an example of the output available from fugashi and UniDic. Note how besides tokenization it includes a variety of information about each token. This is only some of the fields available in UniDic. \n",
    "\n"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup \n",
    "\n",
    "\n",
    "First you'll need to install fugashi and the dictionary.  \n",
    "\n",
    "\n",
    "**fugashi** is a wrapper for **MeCab**, a classic Japanese morphological analyzer. fugashi uses Cython to access MeCab's C interface, and also includes some convenient tweaks to make it easier to use in Python. \n",
    "\n",
    "\n",
    "[wrapper_ja]: 「ラッパー」(英：wrapper, 包み)とは、あるプログラムを「包んで」別の API を提供するプログラムやライブラリのことです。\n",
    "\n",
    "**unidic-lite** is a slightly modified version of UniDic 2.1.2. That version of UniDic is somewhat old, but it's small enough that it's easy to install, and high quality enough that it's sufficient for most applications. The **unidic** package on PyPI wraps the latest edition of UniDic, but due to a large increase in dictionary entries, it's harder to set up, so we won't use it for this tutorial. \n",
    "\n",
    "\n",
    "At time of writing the latest version of fugashi is 1.1.0 and the latest version of unidic-lite is 1.0.8. unidic-lite will work on any system, and fugashi distributes ready-to-use \"wheels\" for OSX, Linux, and 64 bit Windows. (If you have another operating system you may have to build from source. If you have trouble please feel free to [open an issue](https://github.com/polm/fugashi).) \n",
    "\n"]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zfhwH4J0w3Px"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install fugashi unidic-lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJOFlUJpw7nV"
   },
   "source": [
    "Now that fugashi is installed, you can confirm it works by running it in the terminal. Try running `fugashi -O wakati` and then typing some Japanese. If you push Enter, your input text will be printed with spaces separating tokens. You can use `CTRL+D` to terminate the process. Here's some example output: \n",
    "\n"]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QbXeIkMvy8J2",
    "outputId": "a7815e49-9fb8-4534-e3e6-89a14b630dd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "毎年 東 麻布 で は かかし 祭り が 開催 さ れ ます\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"毎年東麻布ではかかし祭りが開催されます\" | fugashi -O wakati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFbndaggzW75"
   },
   "source": [
    "Note: `wakati` comes from 分かち書き *wakachigaki*, which refers to the practice of writing Japanese with spaces included, as used in children's books and low resolution displays. In MeCab this refers to the special output mode that just separates tokens with spaces. Note that real wakachigaki uses spaces to separate bunsetsu, not tokens or words. \n",
    "\n",
    "\n",
    "Next let's use fugashi in code. The main interface to the library is the `Tagger` object, which holds a variety of dictionary related state. The primary way to use the `Tagger` is to simply apply it to input text, which will return a list of `Node` objects. Each `Node` contains the raw text of the token in a `surface` property, and extended dictionary fields are available in the `feature` property. \n",
    "\n"]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vvHy307g0rls",
    "outputId": "157f604e-b2be-49ed-d80b-565b300c2d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[形態, 素, 解析, を, やっ, て, み, た]\n",
      "=====\n",
      "形態\t形態\tケイタイ\n",
      "素\t素\tソ\n",
      "解析\t解析\tカイセキ\n",
      "を\tを\tヲ\n",
      "やっ\t遣る\tヤッ\n",
      "て\tて\tテ\n",
      "み\t見る\tミ\n",
      "た\tた\tタ\n"
     ]
    }
   ],
   "source": [
    "import fugashi\n",
    "\n",
    "tagger = fugashi.Tagger()\n",
    "\n",
    "text = \"形態素解析をやってみた\"\n",
    "words = tagger(text)\n",
    "print(words)\n",
    "print(\"=====\")\n",
    "\n",
    "for word in words:\n",
    "    print(word.surface, word.feature.lemma, word.feature.kana, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Js3MbNVlKDQ"
   },
   "source": [
    "Note: In Japanese NLP, it's standard to refer to the raw input text form as the \"surface\" (表層 *hyousou*), and MeCab uses this in its API. This usage comes from linguistics, where the **surface form** of a word in a particular context (which may be inflected or have unusual orthography) is contrasted with the **lexical form**, which would be a normalized or dictionary form. \n",
    "\n",
    "\n",
    "For basic tokenization, this is all you need to know. In the next section, we'll look at a slightly more involved application of morphological analysis, and later in this chapter we'll cover advanced tokenization-related topics. \n",
    "\n"]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xv7QxAOFpW0o"
   },
   "source": [
    "### Morphological Analysis Mini Project: Automatic Fuseji \n",
    "\n",
    "\n",
    "*Fuseji* (伏せ字) is the practice of replacing some characters with placeholders, usually a circle, to conceal the content of words. A similar thing is sometimes done in English, particularly to avoid using obscene words (`a**hole`, \"you little @#%(*!\"). In Japanese fuseji can be used for obscene words, but they can also be used to avoid spoilers, be vague about the names of brands or specific people, or for other reasons. \n",
    "\n",
    "\n",
    "Let's pretend that we want to automatically apply fuseji for the purpose of hiding spoilers about new movies or other media. While the simplest thing is to replace characters at random from the whole string, it's better to replace certain kinds of words, such as proper nouns. We can use the detailed part of speech information in UniDic, along with word boundaries, to replace proper nouns with fuseji versions. \n",
    "\n"]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1eZYRkisQ-J",
    "outputId": "969c86c5-33f1-427e-f58e-932184b0db9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "犯人は◯◯\n",
      "◯◯タワーの高さは333m\n"
     ]
    }
   ],
   "source": [
    "from fugashi import Tagger\n",
    "from random import sample\n",
    "\n",
    "tagger = Tagger()\n",
    "\n",
    "\n",
    "def fuseji_node(text, ratio=1.0):\n",
    "    \"\"\"This function will take a node from tokenization and actually replace parts of it with filler characters.\"\"\"\n",
    "    ll = len(text)\n",
    "    idxs = sample(range(ll), max(1, int(ratio * ll)))\n",
    "    out = []\n",
    "    for ii, cc in enumerate(text):\n",
    "        out.append(\"◯\" if ii in idxs else cc)\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "def fuseji_text(text, ratio=1.0):\n",
    "    \"\"\"Given an input string, apply fuseji. \"\"\"\n",
    "    out = []\n",
    "    for node in tagger(text):\n",
    "        # Normal Japanese text doesn't use white space, but this is necessary \n",
    "        # if you include latin text, for example. \n",
    "        out.append(node.white_space)\n",
    "        if node.feature.pos2 != \"固有名詞\":\n",
    "            out.append(node.surface)\n",
    "        else:\n",
    "            out.append(fuseji_node(node.surface))\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "print(fuseji_text(\"犯人はヤス\"))\n",
    "print(fuseji_text(\"東京タワーの高さは333m\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is already reasonably effective, but there are several ways it could be tweaked or improved. For example, sometimes the words that should be concealed aren't just proper nouns; they could also be ordinary nouns or verbs. \n",
    "\n",
    "\n",
    "How can we find what parts of speech we want to filter? The best way is to use **example sentences** to find what parts of speech we want, as well as to get a better understanding of where our program works well and where it doesn't. \n",
    "\n",
    "\n",
    "- 新キャラの「カズヤ」は年内に配信予定\n",
    "- マジルテの水晶の畑エリアにはクリスタルが沢山ある\n",
    "- 「吾輩は猫である」の作家は夏目漱石\n",
    "- 『さかしま』（仏: À rebours）は、フランスの作家ジョリス＝カルル・ユイスマンスによる小説\n",
    "\n",
    "We can check the parts of speech of words in fugashi by using the `node.pos` attribute. This part of speech information comes from UniDic and uses four levels. You can access the individual levels as `node.feature.pos1`, `node.feature.pos2`, and so on. The `node.pos` attribute is a convenience feature that joins the four separate values together and replaces empty values with an asterisk (`*`). \n",
    "\n",
    "\n",
    "[^pos_ryaku]: `pos`は「part of speech (品詞）」の略です。\n",
    "\n",
    "You can check part of speech tags of words by giving a sentence as input with fugashi on the command line, without giving the `-O wakati` command line argument. \n",
    "\n"]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "毎年\tマイトシ\tマイトシ\t毎年\t名詞-普通名詞-副詞可能\t\t\t0\r\n",
      "東\tヒガシ\tヒガシ\t東\t名詞-普通名詞-一般\t\t\t0,3\r\n",
      "麻布\tアザブ\tアザブ\tアザブ\t名詞-固有名詞-地名-一般\t\t\t0\r\n",
      "で\tデ\tデ\tで\t助詞-格助詞\t\t\t\r\n",
      "は\tワ\tハ\tは\t助詞-係助詞\t\t\t\r\n",
      "かかし\tカカシ\tカカス\t欠かす\t動詞-一般\t五段-サ行\t連用形-一般\t0,2\r\n",
      "祭り\tマツリ\tマツリ\t祭り\t名詞-普通名詞-一般\t\t\t0\r\n",
      "が\tガ\tガ\tが\t助詞-格助詞\t\t\t\r\n",
      "開催\tカイサイ\tカイサイ\t開催\t名詞-普通名詞-サ変可能\t\t\t0\r\n",
      "さ\tサ\tスル\t為る\t動詞-非自立可能\tサ行変格\t未然形-サ\t0\r\n",
      "れ\tレ\tレル\tれる\t助動詞\t助動詞-レル\t連用形-一般\t\r\n",
      "ます\tマス\tマス\tます\t助動詞\t助動詞-マス\t終止形-一般\t\r\n",
      "EOS\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"毎年東麻布ではかかし祭りが開催されます\" | fugashi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Censoring Unknown Words \n"]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyTaY1txwTd0"
   },
   "source": [
    "Another thing that'll come up as we're testing is that sometimes words not in the dictionary will be used, like the names of characters in movies and books. From the example sentences above, マジルテ *Majirute*, the name of a fictional place, is an example of such a word. We basically always want to censor those words to avoid spoilers, so rather than checking part of speech information, we can also check specifically for words that aren't in our dictionary. These are called \"unks\", from \"unknown words\", or 未知語 *michigo* in Japanese. In fugashi you can determine if a given node is in the dictionary just by checking the `node.is_unk` attribute. \n",
    "\n",
    "\n",
    "Looking at our example sentences, some patterns emerge. We probably don't want to filter verbs, since it's hard to tell when a verb is important. Proper nouns should definitely be filtered. Common nouns may or may not be important, so it's hard to say if we should filter them - for now, let's leave them alone. \n",
    "\n",
    "\n",
    "Since our conditions for censoring words are getting kind of complicated, let's factor them into a function. \n",
    "\n"]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q_mUOEU3wS8c",
    "outputId": "20da4fd9-77df-44b7-ae7e-89a3ee4264d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "犯人は◯◯\n",
      "魔法の言葉は◯◯◯◯◯\n",
      "『さかしま』（仏: ◯ ◯◯◯◯◯◯◯）は、◯◯◯◯の作家◯◯◯◯＝◯◯◯・◯◯◯◯◯◯による小説\n",
      "◯◯爆発で最初に解体する爆弾はみかんの形をしている\n"
     ]
    }
   ],
   "source": [
    "def should_hide(node):\n",
    "    \"\"\"Check if this node should be hidden or not. \"\"\"\n",
    "    if node.is_unk:\n",
    "        return True\n",
    "    ff = node.feature\n",
    "    if ff.pos1 == \"名詞\" and ff.pos2 == \"固有名詞\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def fuseji_text(text, ratio=1.0):\n",
    "    \"\"\"Given an input string, apply fuseji. \"\"\"\n",
    "    out = []\n",
    "    for node in tagger(text):\n",
    "        out.append(node.white_space)\n",
    "        word = fuseji_node(node.surface) if should_hide(node) else node.surface\n",
    "        out.append(word)\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "texts = [\n",
    "    \"犯人はヤス\",\n",
    "    \"魔法の言葉はヒラケゴマ\",\n",
    "    \"『さかしま』（仏: À rebours）は、フランスの作家ジョリス＝カルル・ユイスマンスによる小説\",\n",
    "    \"鈴木爆発で最初に解体する爆弾はみかんの形をしている\",\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print(fuseji_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Readings to Censor only Part of Words \n"]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1n9_7yx4E9h"
   },
   "source": [
    "At this point our program is pretty effective at applying fuseji to any text we throw at it. That said, censoring the entire text is a little boring. It would be more interesting if we could reveal some letters so that readers can guess the rest of the word, but not quite be certain about it. \n",
    "\n",
    "\n",
    "There is one potential issue though - if we use kanji, even one character might give the word away in away that's not interesting. What if we could convert words to phonetic versions and *then* censor part of them? That would allow us to show part of the word while giving away less information. \n",
    "\n",
    "\n",
    "Thankfully, UniDic includes a field we can use for this conversion. Every word in the UniDic dictionary has a `kana` field we can use to get the conventional reading for the word in katakana form. (UniDic also has a `pron` field, which uses non-standard orthography to differentiate long vowels.) \n",
    "\n",
    "\n",
    "One thing to keep in mind is that the kana reading will only be available for words in UniDic, and it won't always be perfect. There are two cases where the reading will be wrong: \n",
    "\n",
    "\n",
    "1. The word is not in the dictionary. \n",
    "2. The reading of the word is ambiguous. \n",
    "\n",
    "\n",
    "If the word is not in the dictionary, it's possible to train a machine learning model or use other methods to predict the reading, but that's pretty difficult. So this time, if a word is an unk we'll just skip converting it and use the raw surface form. \n",
    "\n",
    "\n",
    "Ambiguous words are more difficult. Some examples of ambiguous words: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- 東: *higashi* or *azuma* (or *tou*)\n",
    "- 中田: *nakada* or *nakata*\n",
    "- 仮名: *kana* or *kamei*\n",
    "- 網代: *amishiro* or *ajiro*\n",
    "- 最中: *saichuu* or *monaka*\n",
    "- 私: *watashi* or *watakushi*\n",
    "- 日本: *nihon* or *nippon*\n",
    "\n",
    "d\n",
    "\n",
    "\n",
    "- 東: ひがし、あずま、とう\n",
    "- 中田: なかだ、なかた\n",
    "- 仮名: かな、かめい\n",
    "- 牧場: ぼくじょう、まきば\n",
    "- 網代: あみしろ、あじろ\n",
    "- 日本: にほん、にっぽん\n",
    "\n",
    "d\n",
    "\n",
    "Usually a reading will be clear from context, but many ambiguous words are proper nouns like the names of people and places, and without knowing which specific entity it's referring to there's no way to be sure of the correct reading. Even worse, there's no way to be sure if the word you're looking at is ambiguous or not just using the tokenizer output. \n",
    "\n",
    "\n",
    "(Note: Words written the same way but pronounced differently are referred to as 同形異音語 *doukei iongo* or \"heteronyms\". They are also common in English, though less so for proper nouns.) \n",
    "\n",
    "Note: For ambiguous words, deciding their reading could be considered a form of **word sense disambiguation** for common nouns, or **entity linking** for proper nouns. Both are NLP problems with a long history. \n",
    "\n",
    "\n",
    "So how can we handle ambiguous words if we can't even identify them with certainty? It turns out that their difficulty actually has a silver lining - because even people make mistakes, we can get away with just using the kana UniDic gives us and hope that it's right most of the time. For serious applications replacing the original text with a mistake would be unacceptable, but for our fuseji application, it's not the end of the world if we're wrong occasionally.  \n",
    "\n",
    "\n",
    "Sometimes when you learn about a problem confronting your NLP system, there may not be a solution you're able to implement. In this case, writing a program to disambiguate words would be much more work than the rest of our entire program. But by being aware of the problem, we can consider how failures affect the output of our system, and evaluate whether we should continue with its development, or start over with a design that can work around the problem. \n",
    "\n",
    "\n",
    "Now that we've settled that, let's change our code to use the kana instead of the surface when censoring words. \n",
    "\n"]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F50dN9DcDXcn",
    "outputId": "f57031f6-6b48-4548-8724-f1bbe75c470d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "黒幕の正体は◯ーラ◯ド\n"
     ]
    }
   ],
   "source": [
    "def fuseji_text(text, ratio=1.0):\n",
    "    \"\"\"Given an input string, apply fuseji. \"\"\"\n",
    "    out = []\n",
    "    for node in tagger(text):\n",
    "        out.append(node.white_space)\n",
    "        node_text = node.surface if node.is_unk else node.feature.kana\n",
    "        word = fuseji_node(node_text, ratio=0.5) if should_hide(node) else node.surface\n",
    "        out.append(word)\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "texts = [\n",
    "    \"黒幕の正体はガーランド\",\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print(fuseji_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KK-FXGT4FPjZ"
   },
   "source": [
    "And that makes our automatic fuseji program complete. It's not a lot of code, but in building this you learned how to: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. iterate over the tokens in a text\n",
    "2. identify parts of speech of interest with example sentences\n",
    "3. use multiple levels of part of speech tags\n",
    "4. check if a token is in the dictionary or an unk\n",
    "5. convert words to their phonetic representation\n",
    "\n",
    "d\n",
    "\n",
    "\n",
    "1. 文章の単語を一つずつ処理する方法\n",
    "2. 例文を使って目的の品詞を特定する方法\n",
    "3. 品詞の構造の扱い\n",
    "4. 未知語の判別\n",
    "5. 読み仮名変換\n",
    "\n",
    "d\n",
    "\n",
    "These are all basic building blocks you can use to build a wide variety of applications. \n",
    "\n",
    "\n",
    "While our motivation for this program was a simple and playful one, the techniques used here are simple versions of those used in **personally identifying information (PII) removal**, which removes identifying details from documents like medical and legal records so they can be used in audits or analysis without risk to the people they describe. \n",
    "\n",
    "\n",
    "To learn more about the tokenizer API, consider some ways you might want to extend this application and how you'd make the necessary changes. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- what if you wanted to remove all numbers from a contract, to hide dates or prices?\n",
    "- what if you wanted to hide a specific list of words, perhaps obscenities, rather than certain parts of speech?\n",
    "- how would you change the program to replace hard-to-read words with their phonetic versions?\n",
    "\n",
    "d\n",
    "\n",
    "\n",
    "- 契約書から日付や金額などの数字を消す\n",
    "- 品詞によってではなく、禁止語など特定の単語を伏せる\n",
    "- 難読語を読み仮名に変換する\n",
    "\n",
    "d"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "janlp-ch2.1.2-fugashi-fuseji.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
