{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yr42C927w9w7"
   },
   "source": [
    "\n",
    "## 2.1 fugashi の紹介 \n",
    "\n",
    "\n",
    "この章では、日本語の形態素解析器 MeCab のラッパーである fugashi と、辞書の unidic-lite を使い、日本語の形態素解析の基礎について学びます。 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|surface|pos1|pos2|pos3|lemma|pron|kana|goshu|\n",
    "|-------|----|----|----|-----|----|----|-----|\n",
    "|喫茶|名詞|普通名詞|一般|喫茶|キッサ|キッサ|漢|\n",
    "|店|接尾辞|名詞的|一般|店|テン|テン|漢|\n",
    "|と|助詞|格助詞|*|と|ト|ト|和|\n",
    "|カフェ|名詞|普通名詞|一般|カフェ-cafe|カフェ|カフェ|外|\n",
    "|の|助詞|格助詞|*|の|ノ|ノ|和|\n",
    "|違い|名詞|普通名詞|一般|違い|チガイ|チガイ|和|\n",
    "|は|助詞|係助詞|*|は|ワ|ハ|和|\n",
    "|意外|形状詞|一般|*|意外|イガイ|イガイ|漢|\n",
    "|と|助詞|格助詞|*|と|ト|ト|和|\n",
    "|明確|形状詞|一般|*|明確|メーカク|メイカク|漢|\n",
    "\n",
    "\n",
    "\n",
    "以上の表は fugashi と UniDic の出力の例です。単語分割のみならず、各単語について様々な情報も含まれています。以上で表示されている情報もまた UniDic の情報の一部でしかありません。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 事前準備 \n",
    "\n",
    "\n",
    "まず fugashi とその辞書をインストールする必要があります。 \n",
    "\n",
    "\n",
    "**fugashi** は昔からある、人気の形態素解析器 **MeCab** のラッパー[^wrapper_ja]です。Cython を用いて MeCab の C API を Python から使えるようにした上に、Python から便利に使えるように細かい変更が加えられています。 \n",
    "\n",
    "[wrapper_ja]: 「ラッパー」(英：wrapper, 包み)とは、あるプログラムを「包んで」別の API を提供するプログラムやライブラリのことです。\n",
    "\n",
    "\n",
    "**unidic-lite**は、UniDic 2.1.2 をベースに変更を加えた形態素解析用の辞書です。UniDic のバージョンとしてはやや古いですが、情報の質に問題はなく、後のバージョンと比べてデータ量も少なく、扱いやすいという特徴があります。PyPI の **unidic** パッケージを使えば最新版の UniDic も利用できますが、辞書の見出し語数が大量に増えたせいで環境構築が少しばかり複雑なので、本チュートリアルでは使用しません。\n",
    "\n",
    "\n",
    "本書執筆時での fugashi の最新版は 1.1.0、unidic-lite は 1.0.8 です。unidic-lite は純粋にデータのみから構成されており、どの環境でも利用できます。fugashi は OSX・Linux・Win64 の「wheel」を提供していますので、そのいずれの環境であれば他の事前準備は不要です。(他の環境ではソースからビルドする必要があります。もしビルド中に何か問題がありましたら、お気軽に[issue を立ててください](https://github.com/polm/fugashi)。） "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zfhwH4J0w3Px"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install fugashi unidic-lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJOFlUJpw7nV"
   },
   "source": [
    "\n",
    "これで fugashi がインストールされましたので、正しくインストールされているか確認するために一度シェルから実行してみましょう。まずは、`fugashi -O wakati` を実行し適当な文章を入力してみましょう。文章を入力した後に改行を入力すると、入力文がスペース区切りで返ってきます。出力の確認が終わったら `CTRL+D` でプログラムを終了できます。出力は以下のようになります。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QbXeIkMvy8J2",
    "outputId": "a7815e49-9fb8-4534-e3e6-89a14b630dd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "毎年 東 麻布 で は かかし 祭り が 開催 さ れ ます\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"毎年東麻布ではかかし祭りが開催されます\" | fugashi -O wakati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFbndaggzW75"
   },
   "source": [
    "\n",
    "ここで使ったオプション `wakati` は「分かち書き」のことを指しています。「分かち書き」とは、子供向けの本や、低解像度の画面ゆえにひらがなのみで書かれた文章などで見られる、スペース区切りで書かれた日本語の文章のことです。本来、分かち書きは文節をスペースで区切るのが普通ですが、MeCab での分かち書きは単語 (形態素) 単位で文章を区切ります。 \n",
    "\n",
    "\n",
    "それではコードから fugashi を使ってみましょう。コードでは主に辞書の情報を管理する `Tagger` オブジェクトを使います。 `Tagger` を入力分に適応すると `Node` のリストが返ってきます。 `Node` のテキストは `surface` 属性に格納され、辞書のさまざまな情報は `feature` 属性に格納されています。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vvHy307g0rls",
    "outputId": "157f604e-b2be-49ed-d80b-565b300c2d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[形態, 素, 解析, を, やっ, て, み, た]\n",
      "=====\n",
      "形態\t形態\tケイタイ\n",
      "素\t素\tソ\n",
      "解析\t解析\tカイセキ\n",
      "を\tを\tヲ\n",
      "やっ\t遣る\tヤッ\n",
      "て\tて\tテ\n",
      "み\t見る\tミ\n",
      "た\tた\tタ\n"
     ]
    }
   ],
   "source": [
    "import fugashi\n",
    "\n",
    "tagger = fugashi.Tagger()\n",
    "\n",
    "text = \"形態素解析をやってみた\"\n",
    "words = tagger(text)\n",
    "print(words)\n",
    "print(\"=====\")\n",
    "\n",
    "for word in words:\n",
    "    print(word.surface, word.feature.lemma, word.feature.kana, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Js3MbNVlKDQ"
   },
   "source": [
    "\n",
    "日本語の言語処理では、原文の表記そのままのことを指して「表層」（英：surface）と呼ぶことが多いです。これはもともと言語学の用語で、語形変化や表記の違いなども含めた、原文のままの「表層形式」の他に、辞書の見出しなどで使われる、標準化した「語彙形式」（英語：lexical form）があります。 \n",
    "\n",
    "\n",
    "基本的な単語分割の処理の紹介は以上です。次の節では、もう少し高度な形態素解析の応用を紹介し、その後、更に高度な使い方などを説明します。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xv7QxAOFpW0o"
   },
   "source": [
    "\n",
    "### 形態素解析を使ってみよう　自動伏せ字プログラムの実装 \n",
    "\n",
    "\n",
    "**伏せ字**とは、文章の一部の文字を、他の文字に置き換え、単語の一部を隠すことを指します。個人名や商品名の明言を避けるため、ネタバレを避けるため、検索避けのため、など、様々な目的に使われます。 \n",
    "\n",
    "\n",
    "今回は、映画などの新作作品のネタバレを避けるために、自動的に伏せ字を適用するプログラムを書きたいとしましょう。単に入力文章の一部の文字を伏せたいだけであれば、文字をランダムに置換することもできますが、品詞などに基づいてどこを伏せるかを決めると、結果がより自然になります。UniDic の細かい品詞情報と単語分割の結果を活用し、まずは固有名詞を伏せてみましょう。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1eZYRkisQ-J",
    "outputId": "969c86c5-33f1-427e-f58e-932184b0db9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "犯人は◯◯\n",
      "◯◯タワーの高さは333m\n"
     ]
    }
   ],
   "source": [
    "from fugashi import Tagger\n",
    "from random import sample\n",
    "\n",
    "tagger = Tagger()\n",
    "\n",
    "\n",
    "def fuseji_node(text, ratio=1.0):\n",
    "    \"\"\"分かち書きの結果ノードを受け取り、文字列の一部をランダムに◯で置き換える \"\"\"\n",
    "    ll = len(text)\n",
    "    idxs = sample(range(ll), max(1, int(ratio * ll)))\n",
    "    out = []\n",
    "    for ii, cc in enumerate(text):\n",
    "        out.append(\"◯\" if ii in idxs else cc)\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "def fuseji_text(text, ratio=1.0):\n",
    "    \"\"\"入力文を受け取り、適切なところを伏せ字に置き換える \"\"\"\n",
    "    out = []\n",
    "    for node in tagger(text):\n",
    "        # 純粋な日本語のテキストはスペースを含まないが、英語のテキストと混ぜる場合などに必要になる。 \n",
    "        # スペースはそれ自体でノードにならないので、ここで明示的に追加する必要がある。 \n",
    "        out.append(node.white_space)\n",
    "        if node.feature.pos2 != \"固有名詞\":\n",
    "            out.append(node.surface)\n",
    "        else:\n",
    "            out.append(fuseji_node(node.surface))\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "print(fuseji_text(\"犯人はヤス\"))\n",
    "print(fuseji_text(\"東京タワーの高さは333m\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "このプログラムはこのままでもそれなりに使えますが、改善の余地はまだ残っています。場合によっては固有名詞だけでなく、一般名詞や動詞を伏せたいときもあります。 \n",
    "\n",
    "\n",
    "伏せたい品詞はどうやって特定すれば良いでしょうか。品詞表から調べられることも可能ですが、例文をいくつか実際に解析してみて、その結果を見る方が楽で効果的です。実際データを処理してみると、どんなケースで伏せ字プログラムがうまくいくかどうかがより深く理解できます。 \n",
    "\n",
    "- 新キャラの「カズヤ」は年内に配信予定\n",
    "- マジルテの水晶の畑エリアにはクリスタルが沢山ある\n",
    "- 「吾輩は猫である」の作家は夏目漱石\n",
    "- 『さかしま』（仏: À rebours）は、フランスの作家ジョリス＝カルル・ユイスマンスによる小説\n",
    "\n",
    "\n",
    "fugashi の解析結果の品詞は、属性 `node.pos`[^pos_ryaku] を見ることによって確認できます。この品詞体系は UniDic のものに基づいており、４つのレベルから構成されます。各レベルは、`node.feature.pos1`, `node.feature.pos2`, ... を使って参照できます。`node.pos`は4つ全てのレベルの情報を一つの文字列にまとめた属性です。全てのレベルに情報があるとは限らず、情報がない場合、そのレベルは `*`になります。 \n",
    "\n",
    "[^pos_ryaku]: `pos`は「part of speech (品詞）」の略です。\n",
    "\n",
    "\n",
    "例文の品詞タグはシェルから確認できます。上で試したのとは異なり、`-O wakati` を指定しない場合、各行ごとに、品詞情報などを含む単語の情報が表示されます。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "毎年\tマイトシ\tマイトシ\t毎年\t名詞-普通名詞-副詞可能\t\t\t0\r\n",
      "東\tヒガシ\tヒガシ\t東\t名詞-普通名詞-一般\t\t\t0,3\r\n",
      "麻布\tアザブ\tアザブ\tアザブ\t名詞-固有名詞-地名-一般\t\t\t0\r\n",
      "で\tデ\tデ\tで\t助詞-格助詞\t\t\t\r\n",
      "は\tワ\tハ\tは\t助詞-係助詞\t\t\t\r\n",
      "かかし\tカカシ\tカカス\t欠かす\t動詞-一般\t五段-サ行\t連用形-一般\t0,2\r\n",
      "祭り\tマツリ\tマツリ\t祭り\t名詞-普通名詞-一般\t\t\t0\r\n",
      "が\tガ\tガ\tが\t助詞-格助詞\t\t\t\r\n",
      "開催\tカイサイ\tカイサイ\t開催\t名詞-普通名詞-サ変可能\t\t\t0\r\n",
      "さ\tサ\tスル\t為る\t動詞-非自立可能\tサ行変格\t未然形-サ\t0\r\n",
      "れ\tレ\tレル\tれる\t助動詞\t助動詞-レル\t連用形-一般\t\r\n",
      "ます\tマス\tマス\tます\t助動詞\t助動詞-マス\t終止形-一般\t\r\n",
      "EOS\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"毎年東麻布ではかかし祭りが開催されます\" | fugashi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 未知語を伏せ字に変換する "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyTaY1txwTd0"
   },
   "source": [
    "\n",
    "様々な文を使って形態素解析を試してみると、映画や書籍のキャラクターの名前など、辞書に含まれていない単語に出くわします。上の例文中の「マジルテ」(架空の場所の名前) もその一つです。ネタバレを避けるために、これらの単語も積極的に伏せたいので、この場合品詞ではなく「辞書にないこと」に基づいて伏せ字の対象にします。これら「辞書にない単語」は「未知語」、英語では「unk」(unknownの略)と呼ばれます。fugashi では`node.is_unk` の属性を見ることで、その単語が未知語かどうかが確認できます。 \n",
    "\n",
    "\n",
    "これらの例文を確認していくと、該当する品詞が分かってきます。動詞については、重要なものとそうでないものを品詞だけでは区別することができないので、除外しない方が良いでしょう。逆に、固有名詞は全部伏せ字対象にした方が良さそうです。一般名詞は動詞と同様、伏せ字にしたいものとそうでないものがあるので、とりあえず対象外にしましょう。 \n",
    "\n",
    "\n",
    "伏せ字対象の条件がやや複雑化しているので、ここで一旦関数にまとめましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q_mUOEU3wS8c",
    "outputId": "20da4fd9-77df-44b7-ae7e-89a3ee4264d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "犯人は◯◯\n",
      "魔法の言葉は◯◯◯◯◯\n",
      "『さかしま』（仏: ◯ ◯◯◯◯◯◯◯）は、◯◯◯◯の作家◯◯◯◯＝◯◯◯・◯◯◯◯◯◯による小説\n",
      "◯◯爆発で最初に解体する爆弾はみかんの形をしている\n"
     ]
    }
   ],
   "source": [
    "def should_hide(node):\n",
    "    \"\"\"与えられたノードを伏せ字の対象にするかどうかを返す \"\"\"\n",
    "    if node.is_unk:\n",
    "        return True\n",
    "    ff = node.feature\n",
    "    if ff.pos1 == \"名詞\" and ff.pos2 == \"固有名詞\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def fuseji_text(text, ratio=1.0):\n",
    "    \"\"\"与えられた文字列に対して、伏せ字を適用する \"\"\"\n",
    "    out = []\n",
    "    for node in tagger(text):\n",
    "        out.append(node.white_space)\n",
    "        word = fuseji_node(node.surface) if should_hide(node) else node.surface\n",
    "        out.append(word)\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "texts = [\n",
    "    \"犯人はヤス\",\n",
    "    \"魔法の言葉はヒラケゴマ\",\n",
    "    \"『さかしま』（仏: À rebours）は、フランスの作家ジョリス＝カルル・ユイスマンスによる小説\",\n",
    "    \"鈴木爆発で最初に解体する爆弾はみかんの形をしている\",\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print(fuseji_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 読みを使って単語の一部を伏せ字にする "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1n9_7yx4E9h"
   },
   "source": [
    "\n",
    "この時点で、このプログラムはどんな文章でもそれなりに伏せ字を適用することができます。ただし、伏せ字対象となった単語の全ての文字をただ単に伏せるのは少し味気が無いですね。一部の文字だけを伏せ、読み手が単語の残りを少しだけ推測できるようにすればさらに面白いと思われます。\n",
    "\n",
    "\n",
    "しかし、一部の文字だけを伏せると問題が発生します。漢字の単語だと、文字一つだけで元の単語が分かってしまうこともあり、これも面白くありません。これを回避するために、まず単語を読み仮名に変換し、その読みの一部を伏せればどうでしょう。そうすれば、一部の文字を伏せなくても、そう簡単に原文がバレることはないはずです。 \n",
    "\n",
    "\n",
    "UniDic には読み情報もちゃんと収録されているので、この変換に使うことができます。UniDic では、全ての項目に対して `kana` フィールドがあり、表層をこれに置き換えることによって読み仮名に変換できます。（これとは別に `pron` フィールドもありますが、これは一般的な読み仮名ではなく、棒引き仮名遣いを使った表記のものです。）\n",
    "\n",
    "\n",
    "注意点として、読み情報は UniDic に収録されいる単語にしか使えないことと、収録されている単語であっても、読み方が文脈に照らし合わせていつも正しいとは限らないことがあります。読み仮名が正しくない場合は大きく分けて2つあります。 \n",
    "\n",
    "\n",
    "1. 未知語の場合 \n",
    "2. 読み方が曖昧な単語 \n",
    "\n",
    "\n",
    "未知語の場合、機械学習などを使って読み仮名を判定するプログラムを書くことは可能ですが、簡単ではありません。したがって今回は、未知語はあきらめて表層をそのまま使います。 \n",
    "\n",
    "\n",
    "同形異音語（形は同じでも読み方が曖昧な単語）は未知語よりも対応が難しいです。同形異音語の例には以下のようなものがあります。 \n",
    "\n",
    "\n",
    "- 東: *higashi* or *azuma* (or *tou*)\n",
    "- 中田: *nakada* or *nakata*\n",
    "- 仮名: *kana* or *kamei*\n",
    "- 網代: *amishiro* or *ajiro*\n",
    "- 最中: *saichuu* or *monaka*\n",
    "- 私: *watashi* or *watakushi*\n",
    "- 日本: *nihon* or *nippon*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- 東: ひがし、あずま、とう\n",
    "- 中田: なかだ、なかた\n",
    "- 仮名: かな、かめい\n",
    "- 牧場: ぼくじょう、まきば\n",
    "- 網代: あみしろ、あじろ\n",
    "- 日本: にほん、にっぽん\n",
    "\n",
    "\n",
    "\n",
    "文脈から読み方が分かる場合も多いですが、同形異音語の多くは人名や地名などの固有名詞なので、どの人・場所に対して使われているかを知らないと、読みが判別できないこともあります。更に大変なことに、MeCab の出力を見るだけでは、ある単語の読みが曖昧かどうかは分かりません。 \n",
    "\n",
    "\n",
    "\n",
    "同形異音語の読みの判別は、一般名詞の場合は**語義の曖昧性解消** (word sense disambiguation)、固有名詞の場合は**エンティティ・リンキング** (entity linking)のタスクであると捉えることもできます。これらのタスクは自然言語処理の分野では昔から多くの研究が行われてきた課題です。 \n",
    "\n",
    "\n",
    "同形異音語の読みがきちんと判定できないのであれば、どう処理すれば良いでしょう。実はその曖昧性自体がある種の救いになっています。読み方が曖昧な単語だと、人間でも読みを間違えることがあるので、プログラムが間違った読みを出しても読み手がある程度分かってくれます。読み間違いが許されないような応用も数多くありますが、今回の伏せ字プログラムでは、これはあまり気にしなくても特に問題にはなりません。 \n",
    "\n",
    "\n",
    "NLP システムの開発中に問題に出くわした場合、その問題の解決方法を実装するのが現実的ではない場合があります。例えばこの伏せ字プログラムの場合、同形異音語の読みの曖昧性を解消するプログラムを書くことは可能ですが、おそらく解決に必要な作業量は、伏せ字プログラムの他の部分を上回ってしまうでしょう。ただし、問題を正しく認識していれば、うまくいかない場合が出力にどういう影響を及ぼすかを考え、元のプログラムを改善する方法を閃くこともあります。場合によっては開発を断念せざるを得ないこともありますが、逆に問題を最初から回避する仕組み導入できる場合もあります。\n",
    "\n",
    "\n",
    "それでは早速、読み仮名に変換するところを実装しましょう。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F50dN9DcDXcn",
    "outputId": "f57031f6-6b48-4548-8724-f1bbe75c470d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "黒幕の正体は◯ーラ◯ド\n"
     ]
    }
   ],
   "source": [
    "def fuseji_text(text, ratio=1.0):\n",
    "    \"\"\"入力文に伏せ字を適用する \"\"\"\n",
    "    out = []\n",
    "    for node in tagger(text):\n",
    "        out.append(node.white_space)\n",
    "        node_text = node.surface if node.is_unk else node.feature.kana\n",
    "        word = fuseji_node(node_text, ratio=0.5) if should_hide(node) else node.surface\n",
    "        out.append(word)\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "texts = [\n",
    "    \"黒幕の正体はガーランド\",\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print(fuseji_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KK-FXGT4FPjZ"
   },
   "source": [
    "\n",
    "これで今回の伏せ字プログラムは完成となります。行数は決して多くはありませんが、これを書く過程で、下記の機能の使い方を紹介しました。\n",
    "\n",
    "\n",
    "1. iterate over the tokens in a text\n",
    "2. identify parts of speech of interest with example sentences\n",
    "3. use multiple levels of part of speech tags\n",
    "4. check if a token is in the dictionary or an unk\n",
    "5. convert words to their phonetic representation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. 文章の単語を一つずつ処理する方法\n",
    "2. 例文を使って目的の品詞を特定する方法\n",
    "3. 品詞の構造の扱い\n",
    "4. 未知語の判別\n",
    "5. 読み仮名変換\n",
    "\n",
    "\n",
    "\n",
    "これらの機能はどちらも基礎的な処理なので、組み合わせによってさまざまなプログラムを作ることができます。 \n",
    "\n",
    "\n",
    "今回の伏せ字プログラムは単純で遊び的なものに過ぎませんが、ここで使った技術をさらに発展させ、実用的な個人情報漏洩防止ツールの実装も可能です。このようなツールは、カルテのような医療情報や契約書などの法的文書を監査や解析に出す前に、個人情報を特定・削除するために広く使われています。\n",
    "\n",
    "\n",
    "MeCab の API を更に深く理解するために、下記の場合、どうやってこの伏せ字プログラムを変更するか考えてみましょう。 \n",
    "\n",
    "\n",
    "- what if you wanted to remove all numbers from a contract, to hide dates or prices?\n",
    "- what if you wanted to hide a specific list of words, perhaps obscenities, rather than certain parts of speech?\n",
    "- how would you change the program to replace hard-to-read words with their phonetic versions?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- 契約書から日付や金額などの数字を消す\n",
    "- 品詞によってではなく、禁止語など特定の単語を伏せる\n",
    "- 難読語を読み仮名に変換する\n",
    "\n",
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "janlp-ch2.1.2-fugashi-fuseji.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
