{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yr42C927w9w7"
   },
   "source": [
    "## 2.1 An Introduction to fugashi :en\n",
    "\n",
    "## 2.1 fugashi の紹介 :ja\n",
    "\n",
    "In this section you'll learn how to do Japanese tokenization using fugashi, a MeCab wrapper, and the unidic-lite dictionary. :en\n",
    "\n",
    "この章では、日本語の形態素解析器 MeCab のラッパーである fugashi と、辞書の unidic-lite を使い、日本語の形態素解析の基礎について学びます。 :ja\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|surface|pos1|pos2|pos3|lemma|pron|kana|goshu|\n",
    "|-------|----|----|----|-----|----|----|-----|\n",
    "|喫茶|名詞|普通名詞|一般|喫茶|キッサ|キッサ|漢|\n",
    "|店|接尾辞|名詞的|一般|店|テン|テン|漢|\n",
    "|と|助詞|格助詞|*|と|ト|ト|和|\n",
    "|カフェ|名詞|普通名詞|一般|カフェ-cafe|カフェ|カフェ|外|\n",
    "|の|助詞|格助詞|*|の|ノ|ノ|和|\n",
    "|違い|名詞|普通名詞|一般|違い|チガイ|チガイ|和|\n",
    "|は|助詞|係助詞|*|は|ワ|ハ|和|\n",
    "|意外|形状詞|一般|*|意外|イガイ|イガイ|漢|\n",
    "|と|助詞|格助詞|*|と|ト|ト|和|\n",
    "|明確|形状詞|一般|*|明確|メーカク|メイカク|漢|\n",
    "\n",
    "\n",
    "This table is an example of the output available from fugashi and UniDic. Note how besides tokenization it includes a variety of information about each token. This is only some of the fields available in UniDic. :en\n",
    "\n",
    "以上の表は fugashi と UniDic の出力の例です。単語分割のみならず、各単語について様々な情報も含まれています。以上で表示されている情報もまた UniDic の情報の一部でしかありません。 :ja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup :en\n",
    "\n",
    "### 事前準備 :ja\n",
    "\n",
    "First you'll need to install fugashi and the dictionary.  :en\n",
    "\n",
    "まず fugashi とその辞書をインストールする必要があります。 :ja\n",
    "\n",
    "**fugashi** is a wrapper for **MeCab**, a classic Japanese morphological analyzer. fugashi uses Cython to access MeCab's C interface, and also includes some convenient tweaks to make it easier to use in Python. :en\n",
    "\n",
    "**fugashi** は昔からある、人気の形態素解析器 **MeCab** のラッパー[^wrapper_ja]です。Cython を用いて MeCab の C API を Python から使えるようにした上に、Python から便利に使えるように細かい変更が加えられています。 :ja\n",
    "\n",
    "[wrapper_ja]: 「ラッパー」(英：wrapper, 包み)とは、あるプログラムを「包んで」別の API を提供するプログラムやライブラリのことです。\n",
    "\n",
    "**unidic-lite** is a slightly modified version of UniDic 2.1.2. That version of UniDic is somewhat old, but it's small enough that it's easy to install, and high quality enough that it's sufficient for most applications. The **unidic** package on PyPI wraps the latest edition of UniDic, but due to a large increase in dictionary entries, it's harder to set up, so we won't use it for this tutorial. :en\n",
    "\n",
    "**unidic-lite**は、UniDic 2.1.2 をベースに変更を加えた形態素解析用の辞書です。UniDic のバージョンとしてはやや古いですが、情報の質に問題はなく、後のバージョンと比べてデータ量も少なく、扱いやすいという特徴があります。PyPI の **unidic** パッケージを使えば最新版の UniDic も利用できますが、辞書の見出し語数が大量に増えたせいで環境構築が少しばかり複雑なので、本チュートリアルでは使用しません。:ja\n",
    "\n",
    "At time of writing the latest version of fugashi is 1.1.0 and the latest version of unidic-lite is 1.0.8. unidic-lite will work on any system, and fugashi distributes ready-to-use \"wheels\" for OSX, Linux, and 64 bit Windows. (If you have another operating system you may have to build from source. If you have trouble please feel free to [open an issue](https://github.com/polm/fugashi).) :en\n",
    "\n",
    "本書執筆時での fugashi の最新版は 1.1.0、unidic-lite は 1.0.8 です。unidic-lite は純粋にデータのみから構成されており、どの環境でも利用できます。fugashi は OSX・Linux・Win64 の「wheel」を提供していますので、そのいずれの環境であれば他の事前準備は不要です。(他の環境ではソースからビルドする必要があります。もしビルド中に何か問題がありましたら、お気軽に[issue を立ててください](https://github.com/polm/fugashi)。） :ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zfhwH4J0w3Px"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install fugashi unidic-lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJOFlUJpw7nV"
   },
   "source": [
    "Now that fugashi is installed, you can confirm it works by running it in the terminal. Try running `fugashi -O wakati` and then typing some Japanese. If you push Enter, your input text will be printed with spaces separating tokens. You can use `CTRL+D` to terminate the process. Here's some example output: :en\n",
    "\n",
    "これで fugashi がインストールされましたので、正しくインストールされているか確認するために一度シェルから実行してみましょう。まずは、`fugashi -O wakati` を実行し適当な文章を入力してみましょう。文章を入力した後に改行を入力すると、入力文がスペース区切りで返ってきます。出力の確認が終わったら `CTRL+D` でプログラムを終了できます。出力は以下のようになります。 :ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QbXeIkMvy8J2",
    "outputId": "a7815e49-9fb8-4534-e3e6-89a14b630dd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "毎年 東 麻布 で は かかし 祭り が 開催 さ れ ます\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"毎年東麻布ではかかし祭りが開催されます\" | fugashi -O wakati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFbndaggzW75"
   },
   "source": [
    "Note: `wakati` comes from 分かち書き *wakachigaki*, which refers to the practice of writing Japanese with spaces included, as used in children's books and low resolution displays. In MeCab this refers to the special output mode that just separates tokens with spaces. Note that real wakachigaki uses spaces to separate bunsetsu, not tokens or words. :en\n",
    "\n",
    "ここで使ったオプション `wakati` は「分かち書き」のことを指しています。「分かち書き」とは、子供向けの本や、低解像度の画面ゆえにひらがなのみで書かれた文章などで見られる、スペース区切りで書かれた日本語の文章のことです。本来、分かち書きは文節をスペースで区切るのが普通ですが、MeCab での分かち書きは単語 (形態素) 単位で文章を区切ります。 :ja\n",
    "\n",
    "Next let's use fugashi in code. The main interface to the library is the `Tagger` object, which holds a variety of dictionary related state. The primary way to use the `Tagger` is to simply apply it to input text, which will return a list of `Node` objects. Each `Node` contains the raw text of the token in a `surface` property, and extended dictionary fields are available in the `feature` property. :en\n",
    "\n",
    "それではコードから fugashi を使ってみましょう。コードでは主に辞書の情報を管理する `Tagger` オブジェクトを使います。 `Tagger` を入力分に適応すると `Node` のリストが返ってきます。 `Node` のテキストは `surface` 属性に格納され、辞書のさまざまな情報は `feature` 属性に格納されています。 :ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vvHy307g0rls",
    "outputId": "157f604e-b2be-49ed-d80b-565b300c2d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[形態, 素, 解析, を, やっ, て, み, た]\n",
      "=====\n",
      "形態\t形態\tケイタイ\n",
      "素\t素\tソ\n",
      "解析\t解析\tカイセキ\n",
      "を\tを\tヲ\n",
      "やっ\t遣る\tヤッ\n",
      "て\tて\tテ\n",
      "み\t見る\tミ\n",
      "た\tた\tタ\n"
     ]
    }
   ],
   "source": [
    "import fugashi\n",
    "\n",
    "tagger = fugashi.Tagger()\n",
    "\n",
    "text = \"形態素解析をやってみた\"\n",
    "words = tagger(text)\n",
    "print(words)\n",
    "print(\"=====\")\n",
    "\n",
    "for word in words:\n",
    "    print(word.surface, word.feature.lemma, word.feature.kana, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Js3MbNVlKDQ"
   },
   "source": [
    "Note: In Japanese NLP, it's standard to refer to the raw input text form as the \"surface\" (表層 *hyousou*), and MeCab uses this in its API. This usage comes from linguistics, where the **surface form** of a word in a particular context (which may be inflected or have unusual orthography) is contrasted with the **lexical form**, which would be a normalized or dictionary form. :en\n",
    "\n",
    "日本語の言語処理では、原文の表記そのままのことを指して「表層」（英：surface）と呼ぶことが多いです。これはもともと言語学の用語で、語形変化や表記の違いなども含めた、原文のままの「表層形式」の他に、辞書の見出しなどで使われる、標準化した「語彙形式」（英語：lexical form）があります。 :ja\n",
    "\n",
    "For basic tokenization, this is all you need to know. In the next section, we'll look at a slightly more involved application of morphological analysis, and later in this chapter we'll cover advanced tokenization-related topics. :en\n",
    "\n",
    "基本的な単語分割の処理の紹介は以上です。次の節では、もう少し高度な形態素解析の応用を紹介し、その後、更に高度な使い方などを説明します。 :ja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xv7QxAOFpW0o"
   },
   "source": [
    "### Morphological Analysis Mini Project: Automatic Fuseji :en\n",
    "\n",
    "### 形態素解析を使ってみよう　自動伏せ字プログラムの実装 :ja\n",
    "\n",
    "*Fuseji* (伏せ字) is the practice of replacing some characters with placeholders, usually a circle, to conceal the content of words. A similar thing is sometimes done in English, particularly to avoid using obscene words (`a**hole`, \"you little @#%(*!\"). In Japanese fuseji can be used for obscene words, but they can also be used to avoid spoilers, be vague about the names of brands or specific people, or for other reasons. :en\n",
    "\n",
    "**伏せ字**とは、文章の一部の文字を、他の文字に置き換え、単語の一部を隠すことを指します。個人名や商品名の明言を避けるため、ネタバレを避けるため、検索避けのため、など、様々な目的に使われます。 :ja\n",
    "\n",
    "Let's pretend that we want to automatically apply fuseji for the purpose of hiding spoilers about new movies or other media. While the simplest thing is to replace characters at random from the whole string, it's better to replace certain kinds of words, such as proper nouns. We can use the detailed part of speech information in UniDic, along with word boundaries, to replace proper nouns with fuseji versions. :en\n",
    "\n",
    "今回は、映画などの新作作品のネタバレを避けるために、自動的に伏せ字を適用するプログラムを書きたいとしましょう。単に入力文章の一部の文字を伏せたいだけであれば、文字をランダムに置換することもできますが、品詞などに基づいてどこを伏せるかを決めると、結果がより自然になります。UniDic の細かい品詞情報と単語分割の結果を活用し、まずは固有名詞を伏せてみましょう。 :ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1eZYRkisQ-J",
    "outputId": "969c86c5-33f1-427e-f58e-932184b0db9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "犯人は◯◯\n",
      "◯◯タワーの高さは333m\n"
     ]
    }
   ],
   "source": [
    "from fugashi import Tagger\n",
    "from random import sample\n",
    "\n",
    "tagger = Tagger()\n",
    "\n",
    "\n",
    "def fuseji_node(text, ratio=1.0):\n",
    "    \"\"\"This function will take a node from tokenization and actually replace parts of it with filler characters.:en\"\"\"\n",
    "    \"\"\"分かち書きの結果ノードを受け取り、文字列の一部をランダムに◯で置き換える :ja\"\"\"\n",
    "    ll = len(text)\n",
    "    idxs = sample(range(ll), max(1, int(ratio * ll)))\n",
    "    out = []\n",
    "    for ii, cc in enumerate(text):\n",
    "        out.append(\"◯\" if ii in idxs else cc)\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "def fuseji_text(text, ratio=1.0):\n",
    "    \"\"\"Given an input string, apply fuseji. :en\"\"\"\n",
    "    \"\"\"入力文を受け取り、適切なところを伏せ字に置き換える :ja\"\"\"\n",
    "    out = []\n",
    "    for node in tagger(text):\n",
    "        # Normal Japanese text doesn't use white space, but this is necessary :en\n",
    "        # if you include latin text, for example. :en\n",
    "        # 純粋な日本語のテキストはスペースを含まないが、英語のテキストと混ぜる場合などに必要になる。 :ja\n",
    "        # スペースはそれ自体でノードにならないので、ここで明示的に追加する必要がある。 :ja\n",
    "        out.append(node.white_space)\n",
    "        if node.feature.pos2 != \"固有名詞\":\n",
    "            out.append(node.surface)\n",
    "        else:\n",
    "            out.append(fuseji_node(node.surface))\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "print(fuseji_text(\"犯人はヤス\"))\n",
    "print(fuseji_text(\"東京タワーの高さは333m\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is already reasonably effective, but there are several ways it could be tweaked or improved. For example, sometimes the words that should be concealed aren't just proper nouns; they could also be ordinary nouns or verbs. :en\n",
    "\n",
    "このプログラムはこのままでもそれなりに使えますが、改善の余地はまだ残っています。場合によっては固有名詞だけでなく、一般名詞や動詞を伏せたいときもあります。 :ja\n",
    "\n",
    "How can we find what parts of speech we want to filter? The best way is to use **example sentences** to find what parts of speech we want, as well as to get a better understanding of where our program works well and where it doesn't. :en\n",
    "\n",
    "伏せたい品詞はどうやって特定すれば良いでしょうか。品詞表から調べられることも可能ですが、例文をいくつか実際に解析してみて、その結果を見る方が楽で効果的です。実際データを処理してみると、どんなケースで伏せ字プログラムがうまくいくかどうかがより深く理解できます。 :ja\n",
    "\n",
    "This list is the same in both languages :xx\n",
    "- 新キャラの「カズヤ」は年内に配信予定\n",
    "- マジルテの水晶の畑エリアにはクリスタルが沢山ある\n",
    "- 「吾輩は猫である」の作家は夏目漱石\n",
    "- 『さかしま』（仏: À rebours）は、フランスの作家ジョリス＝カルル・ユイスマンスによる小説\n",
    "\n",
    "We can check the parts of speech of words in fugashi by using the `node.pos` attribute. This part of speech information comes from UniDic and uses four levels. You can access the individual levels as `node.feature.pos1`, `node.feature.pos2`, and so on. The `node.pos` attribute is a convenience feature that joins the four separate values together and replaces empty values with an asterisk (`*`). :en\n",
    "\n",
    "fugashi の解析結果の品詞は、属性 `node.pos`[^pos_ryaku] を見ることによって確認できます。この品詞体系は UniDic のものに基づいており、４つのレベルから構成されます。各レベルは、`node.feature.pos1`, `node.feature.pos2`, ... を使って参照できます。`node.pos`は4つ全てのレベルの情報を一つの文字列にまとめた属性です。全てのレベルに情報があるとは限らず、情報がない場合、そのレベルは `*`になります。 :ja\n",
    "\n",
    "[^pos_ryaku]: `pos`は「part of speech (品詞）」の略です。\n",
    "\n",
    "You can check part of speech tags of words by giving a sentence as input with fugashi on the command line, without giving the `-O wakati` command line argument. :en\n",
    "\n",
    "例文の品詞タグはシェルから確認できます。上で試したのとは異なり、`-O wakati` を指定しない場合、各行ごとに、品詞情報などを含む単語の情報が表示されます。 :ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "毎年\tマイトシ\tマイトシ\t毎年\t名詞-普通名詞-副詞可能\t\t\t0\r\n",
      "東\tヒガシ\tヒガシ\t東\t名詞-普通名詞-一般\t\t\t0,3\r\n",
      "麻布\tアザブ\tアザブ\tアザブ\t名詞-固有名詞-地名-一般\t\t\t0\r\n",
      "で\tデ\tデ\tで\t助詞-格助詞\t\t\t\r\n",
      "は\tワ\tハ\tは\t助詞-係助詞\t\t\t\r\n",
      "かかし\tカカシ\tカカス\t欠かす\t動詞-一般\t五段-サ行\t連用形-一般\t0,2\r\n",
      "祭り\tマツリ\tマツリ\t祭り\t名詞-普通名詞-一般\t\t\t0\r\n",
      "が\tガ\tガ\tが\t助詞-格助詞\t\t\t\r\n",
      "開催\tカイサイ\tカイサイ\t開催\t名詞-普通名詞-サ変可能\t\t\t0\r\n",
      "さ\tサ\tスル\t為る\t動詞-非自立可能\tサ行変格\t未然形-サ\t0\r\n",
      "れ\tレ\tレル\tれる\t助動詞\t助動詞-レル\t連用形-一般\t\r\n",
      "ます\tマス\tマス\tます\t助動詞\t助動詞-マス\t終止形-一般\t\r\n",
      "EOS\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"毎年東麻布ではかかし祭りが開催されます\" | fugashi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Censoring Unknown Words :en\n",
    "### 未知語を伏せ字に変換する :ja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyTaY1txwTd0"
   },
   "source": [
    "Another thing that'll come up as we're testing is that sometimes words not in the dictionary will be used, like the names of characters in movies and books. From the example sentences above, マジルテ *Majirute*, the name of a fictional place, is an example of such a word. We basically always want to censor those words to avoid spoilers, so rather than checking part of speech information, we can also check specifically for words that aren't in our dictionary. These are called \"unks\", from \"unknown words\", or 未知語 *michigo* in Japanese. In fugashi you can determine if a given node is in the dictionary just by checking the `node.is_unk` attribute. :en\n",
    "\n",
    "様々な文を使って形態素解析を試してみると、映画や書籍のキャラクターの名前など、辞書に含まれていない単語に出くわします。上の例文中の「マジルテ」(架空の場所の名前) もその一つです。ネタバレを避けるために、これらの単語も積極的に伏せたいので、この場合品詞ではなく「辞書にないこと」に基づいて伏せ字の対象にします。これら「辞書にない単語」は「未知語」、英語では「unk」(unknownの略)と呼ばれます。fugashi では`node.is_unk` の属性を見ることで、その単語が未知語かどうかが確認できます。 :ja\n",
    "\n",
    "Looking at our example sentences, some patterns emerge. We probably don't want to filter verbs, since it's hard to tell when a verb is important. Proper nouns should definitely be filtered. Common nouns may or may not be important, so it's hard to say if we should filter them - for now, let's leave them alone. :en\n",
    "\n",
    "これらの例文を確認していくと、該当する品詞が分かってきます。動詞については、重要なものとそうでないものを品詞だけでは区別することができないので、除外しない方が良いでしょう。逆に、固有名詞は全部伏せ字対象にした方が良さそうです。一般名詞は動詞と同様、伏せ字にしたいものとそうでないものがあるので、とりあえず対象外にしましょう。 :ja\n",
    "\n",
    "Since our conditions for censoring words are getting kind of complicated, let's factor them into a function. :en\n",
    "\n",
    "伏せ字対象の条件がやや複雑化しているので、ここで一旦関数にまとめましょう。:ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q_mUOEU3wS8c",
    "outputId": "20da4fd9-77df-44b7-ae7e-89a3ee4264d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "犯人は◯◯\n",
      "魔法の言葉は◯◯◯◯◯\n",
      "『さかしま』（仏: ◯ ◯◯◯◯◯◯◯）は、◯◯◯◯の作家◯◯◯◯＝◯◯◯・◯◯◯◯◯◯による小説\n",
      "◯◯爆発で最初に解体する爆弾はみかんの形をしている\n"
     ]
    }
   ],
   "source": [
    "def should_hide(node):\n",
    "    \"\"\"Check if this node should be hidden or not. :en\"\"\"\n",
    "    \"\"\"与えられたノードを伏せ字の対象にするかどうかを返す :ja\"\"\"\n",
    "    if node.is_unk:\n",
    "        return True\n",
    "    ff = node.feature\n",
    "    if ff.pos1 == \"名詞\" and ff.pos2 == \"固有名詞\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def fuseji_text(text, ratio=1.0):\n",
    "    \"\"\"Given an input string, apply fuseji. :en\"\"\"\n",
    "    \"\"\"与えられた文字列に対して、伏せ字を適用する :ja\"\"\"\n",
    "    out = []\n",
    "    for node in tagger(text):\n",
    "        out.append(node.white_space)\n",
    "        word = fuseji_node(node.surface) if should_hide(node) else node.surface\n",
    "        out.append(word)\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "texts = [\n",
    "    \"犯人はヤス\",\n",
    "    \"魔法の言葉はヒラケゴマ\",\n",
    "    \"『さかしま』（仏: À rebours）は、フランスの作家ジョリス＝カルル・ユイスマンスによる小説\",\n",
    "    \"鈴木爆発で最初に解体する爆弾はみかんの形をしている\",\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print(fuseji_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Readings to Censor only Part of Words :en\n",
    "### 読みを使って単語の一部を伏せ字にする :ja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1n9_7yx4E9h"
   },
   "source": [
    "At this point our program is pretty effective at applying fuseji to any text we throw at it. That said, censoring the entire text is a little boring. It would be more interesting if we could reveal some letters so that readers can guess the rest of the word, but not quite be certain about it. :en\n",
    "\n",
    "この時点で、このプログラムはどんな文章でもそれなりに伏せ字を適用することができます。ただし、伏せ字対象となった単語の全ての文字をただ単に伏せるのは少し味気が無いですね。一部の文字だけを伏せ、読み手が単語の残りを少しだけ推測できるようにすればさらに面白いと思われます。:ja\n",
    "\n",
    "There is one potential issue though - if we use kanji, even one character might give the word away in away that's not interesting. What if we could convert words to phonetic versions and *then* censor part of them? That would allow us to show part of the word while giving away less information. :en\n",
    "\n",
    "しかし、一部の文字だけを伏せると問題が発生します。漢字の単語だと、文字一つだけで元の単語が分かってしまうこともあり、これも面白くありません。これを回避するために、まず単語を読み仮名に変換し、その読みの一部を伏せればどうでしょう。そうすれば、一部の文字を伏せなくても、そう簡単に原文がバレることはないはずです。 :ja\n",
    "\n",
    "Thankfully, UniDic includes a field we can use for this conversion. Every word in the UniDic dictionary has a `kana` field we can use to get the conventional reading for the word in katakana form. (UniDic also has a `pron` field, which uses non-standard orthography to differentiate long vowels.) :en\n",
    "\n",
    "UniDic には読み情報もちゃんと収録されているので、この変換に使うことができます。UniDic では、全ての項目に対して `kana` フィールドがあり、表層をこれに置き換えることによって読み仮名に変換できます。（これとは別に `pron` フィールドもありますが、これは一般的な読み仮名ではなく、棒引き仮名遣いを使った表記のものです。）:ja\n",
    "\n",
    "One thing to keep in mind is that the kana reading will only be available for words in UniDic, and it won't always be perfect. There are two cases where the reading will be wrong: :en\n",
    "\n",
    "注意点として、読み情報は UniDic に収録されいる単語にしか使えないことと、収録されている単語であっても、読み方が文脈に照らし合わせていつも正しいとは限らないことがあります。読み仮名が正しくない場合は大きく分けて2つあります。 :ja\n",
    "\n",
    "1. The word is not in the dictionary. :en\n",
    "2. The reading of the word is ambiguous. :en\n",
    "\n",
    "1. 未知語の場合 :ja\n",
    "2. 読み方が曖昧な単語 :ja\n",
    "\n",
    "If the word is not in the dictionary, it's possible to train a machine learning model or use other methods to predict the reading, but that's pretty difficult. So this time, if a word is an unk we'll just skip converting it and use the raw surface form. :en\n",
    "\n",
    "未知語の場合、機械学習などを使って読み仮名を判定するプログラムを書くことは可能ですが、簡単ではありません。したがって今回は、未知語はあきらめて表層をそのまま使います。 :ja\n",
    "\n",
    "Ambiguous words are more difficult. Some examples of ambiguous words: :en\n",
    "\n",
    "同形異音語（形は同じでも読み方が曖昧な単語）は未知語よりも対応が難しいです。同形異音語の例には以下のようなものがあります。 :ja\n",
    "\n",
    ":en\n",
    "\n",
    "- 東: *higashi* or *azuma* (or *tou*)\n",
    "- 中田: *nakada* or *nakata*\n",
    "- 仮名: *kana* or *kamei*\n",
    "- 網代: *amishiro* or *ajiro*\n",
    "- 最中: *saichuu* or *monaka*\n",
    "- 私: *watashi* or *watakushi*\n",
    "- 日本: *nihon* or *nippon*\n",
    "\n",
    ":end\n",
    "\n",
    ":ja\n",
    "\n",
    "- 東: ひがし、あずま、とう\n",
    "- 中田: なかだ、なかた\n",
    "- 仮名: かな、かめい\n",
    "- 牧場: ぼくじょう、まきば\n",
    "- 網代: あみしろ、あじろ\n",
    "- 日本: にほん、にっぽん\n",
    "\n",
    ":end\n",
    "\n",
    "Usually a reading will be clear from context, but many ambiguous words are proper nouns like the names of people and places, and without knowing which specific entity it's referring to there's no way to be sure of the correct reading. Even worse, there's no way to be sure if the word you're looking at is ambiguous or not just using the tokenizer output. :en\n",
    "\n",
    "文脈から読み方が分かる場合も多いですが、同形異音語の多くは人名や地名などの固有名詞なので、どの人・場所に対して使われているかを知らないと、読みが判別できないこともあります。更に大変なことに、MeCab の出力を見るだけでは、ある単語の読みが曖昧かどうかは分かりません。 :ja\n",
    "\n",
    "(Note: Words written the same way but pronounced differently are referred to as 同形異音語 *doukei iongo* or \"heteronyms\". They are also common in English, though less so for proper nouns.) :en\n",
    "\n",
    "Note: For ambiguous words, deciding their reading could be considered a form of **word sense disambiguation** for common nouns, or **entity linking** for proper nouns. Both are NLP problems with a long history. :en\n",
    "\n",
    "同形異音語の読みの判別は、一般名詞の場合は**語義の曖昧性解消** (word sense disambiguation)、固有名詞の場合は**エンティティ・リンキング** (entity linking)のタスクであると捉えることもできます。これらのタスクは自然言語処理の分野では昔から多くの研究が行われてきた課題です。 :ja\n",
    "\n",
    "So how can we handle ambiguous words if we can't even identify them with certainty? It turns out that their difficulty actually has a silver lining - because even people make mistakes, we can get away with just using the kana UniDic gives us and hope that it's right most of the time. For serious applications replacing the original text with a mistake would be unacceptable, but for our fuseji application, it's not the end of the world if we're wrong occasionally.  :en\n",
    "\n",
    "同形異音語の読みがきちんと判定できないのであれば、どう処理すれば良いでしょう。実はその曖昧性自体がある種の救いになっています。読み方が曖昧な単語だと、人間でも読みを間違えることがあるので、プログラムが間違った読みを出しても読み手がある程度分かってくれます。読み間違いが許されないような応用も数多くありますが、今回の伏せ字プログラムでは、これはあまり気にしなくても特に問題にはなりません。 :ja\n",
    "\n",
    "Sometimes when you learn about a problem confronting your NLP system, there may not be a solution you're able to implement. In this case, writing a program to disambiguate words would be much more work than the rest of our entire program. But by being aware of the problem, we can consider how failures affect the output of our system, and evaluate whether we should continue with its development, or start over with a design that can work around the problem. :en\n",
    "\n",
    "NLP システムの開発中に問題に出くわした場合、その問題の解決方法を実装するのが現実的ではない場合があります。例えばこの伏せ字プログラムの場合、同形異音語の読みの曖昧性を解消するプログラムを書くことは可能ですが、おそらく解決に必要な作業量は、伏せ字プログラムの他の部分を上回ってしまうでしょう。ただし、問題を正しく認識していれば、うまくいかない場合が出力にどういう影響を及ぼすかを考え、元のプログラムを改善する方法を閃くこともあります。場合によっては開発を断念せざるを得ないこともありますが、逆に問題を最初から回避する仕組み導入できる場合もあります。:ja\n",
    "\n",
    "Now that we've settled that, let's change our code to use the kana instead of the surface when censoring words. :en\n",
    "\n",
    "それでは早速、読み仮名に変換するところを実装しましょう。 :ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F50dN9DcDXcn",
    "outputId": "f57031f6-6b48-4548-8724-f1bbe75c470d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "黒幕の正体は◯ーラ◯ド\n"
     ]
    }
   ],
   "source": [
    "def fuseji_text(text, ratio=1.0):\n",
    "    \"\"\"Given an input string, apply fuseji. :en\"\"\"\n",
    "    \"\"\"入力文に伏せ字を適用する :ja\"\"\"\n",
    "    out = []\n",
    "    for node in tagger(text):\n",
    "        out.append(node.white_space)\n",
    "        node_text = node.surface if node.is_unk else node.feature.kana\n",
    "        word = fuseji_node(node_text, ratio=0.5) if should_hide(node) else node.surface\n",
    "        out.append(word)\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "texts = [\n",
    "    \"黒幕の正体はガーランド\",\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print(fuseji_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KK-FXGT4FPjZ"
   },
   "source": [
    "And that makes our automatic fuseji program complete. It's not a lot of code, but in building this you learned how to: :en\n",
    "\n",
    "これで今回の伏せ字プログラムは完成となります。行数は決して多くはありませんが、これを書く過程で、下記の機能の使い方を紹介しました。:ja\n",
    "\n",
    ":en\n",
    "\n",
    "1. iterate over the tokens in a text\n",
    "2. identify parts of speech of interest with example sentences\n",
    "3. use multiple levels of part of speech tags\n",
    "4. check if a token is in the dictionary or an unk\n",
    "5. convert words to their phonetic representation\n",
    "\n",
    ":end\n",
    "\n",
    ":ja\n",
    "\n",
    "1. 文章の単語を一つずつ処理する方法\n",
    "2. 例文を使って目的の品詞を特定する方法\n",
    "3. 品詞の構造の扱い\n",
    "4. 未知語の判別\n",
    "5. 読み仮名変換\n",
    "\n",
    ":end\n",
    "\n",
    "These are all basic building blocks you can use to build a wide variety of applications. :en\n",
    "\n",
    "これらの機能はどちらも基礎的な処理なので、組み合わせによってさまざまなプログラムを作ることができます。 :ja\n",
    "\n",
    "While our motivation for this program was a simple and playful one, the techniques used here are simple versions of those used in **personally identifying information (PII) removal**, which removes identifying details from documents like medical and legal records so they can be used in audits or analysis without risk to the people they describe. :en\n",
    "\n",
    "今回の伏せ字プログラムは単純で遊び的なものに過ぎませんが、ここで使った技術をさらに発展させ、実用的な個人情報漏洩防止ツールの実装も可能です。このようなツールは、カルテのような医療情報や契約書などの法的文書を監査や解析に出す前に、個人情報を特定・削除するために広く使われています。:ja\n",
    "\n",
    "To learn more about the tokenizer API, consider some ways you might want to extend this application and how you'd make the necessary changes. :en\n",
    "\n",
    "MeCab の API を更に深く理解するために、下記の場合、どうやってこの伏せ字プログラムを変更するか考えてみましょう。 :ja\n",
    "\n",
    ":en\n",
    "\n",
    "- what if you wanted to remove all numbers from a contract, to hide dates or prices?\n",
    "- what if you wanted to hide a specific list of words, perhaps obscenities, rather than certain parts of speech?\n",
    "- how would you change the program to replace hard-to-read words with their phonetic versions?\n",
    "\n",
    ":end\n",
    "\n",
    ":ja\n",
    "\n",
    "- 契約書から日付や金額などの数字を消す\n",
    "- 品詞によってではなく、禁止語など特定の単語を伏せる\n",
    "- 難読語を読み仮名に変換する\n",
    "\n",
    ":end"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "janlp-ch2.1.2-fugashi-fuseji.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
